{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "1. [Initialization](#imports)\n",
    "2. [Euthyroid data](#euthyroid)\n",
    "    1. [Settings](#exp_set)\n",
    "    2. [Experiments with decision maker](#exp_dm)\n",
    "    3. [All and free variables](#exp_all)\n",
    "3. [NHANES data](#nhanes)\n",
    "    1. [Settings](#nh_set)\n",
    "    2. [Experiments with decision maker](#nh_dm)\n",
    "    3. [All and free variables](#nh_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization <a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decision_maker import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import itertools\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used for generation of masked and extended training data \n",
    "# - used for pretraining one classifier accepting any combination of acquired variables\n",
    "\n",
    "def get_boots_data(df, z_init, dupl):\n",
    "    # generate list of all possible masks\n",
    "    print('masking started')\n",
    "    is_free = np.where(np.array(z_init)==-1, True, False)\n",
    "    is_free_df = df[:, is_free]\n",
    "    is_costly_df = df[:, ~is_free]\n",
    "\n",
    "    is_zero_indx = list(range(is_costly_df.shape[1]))\n",
    "    is_zero_pwset = itertools.chain.from_iterable(\n",
    "        itertools.combinations(is_zero_indx, r) for r in range(len(is_zero_indx) + 1))\n",
    "\n",
    "    masks = []\n",
    "    for idx in is_zero_pwset:\n",
    "        z_np = np.zeros(is_costly_df.shape[1])\n",
    "        z_np[list(idx)] = 1\n",
    "        masks.append(z_np)\n",
    "\n",
    "    # create empty data\n",
    "    mask_dfs = {}\n",
    "    costly_dfs = {}\n",
    "    free_dfs = {}\n",
    "    for i in range(dupl):\n",
    "        mask_dfs[i] = np.zeros(is_costly_df.shape)\n",
    "        costly_dfs[i] = is_costly_df.copy()\n",
    "        free_dfs[i] = is_free_df.copy()\n",
    "\n",
    "    # for each data row sample different masks\n",
    "    k = len(masks)\n",
    "    for i in range(is_costly_df.shape[0]):\n",
    "        sampled_indx = np.random.choice(range(k), dupl, replace=False)\n",
    "        for d in range(dupl):\n",
    "            mask_dfs[d][i] = masks[sampled_indx[d]]\n",
    "\n",
    "    # concatenate dfs from dictionaries into one dataframe\n",
    "    mask_final_df = np.concatenate(list(mask_dfs.values()), axis=0)\n",
    "\n",
    "    final_costly_df = np.concatenate(list(costly_dfs.values()), axis=0)\n",
    "    final_costly_df = np.where(mask_final_df == 1, 0, final_costly_df)\n",
    "\n",
    "    final_free_df = np.concatenate(list(free_dfs.values()), axis=0)\n",
    "\n",
    "    final_df = np.concatenate([final_costly_df, mask_final_df], axis=1)\n",
    "    final_df = np.concatenate([final_free_df, final_df], axis=1)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to mask data to be compatable with the pre-trained classifier\n",
    "\n",
    "def mask_df(df, mask):\n",
    "    # mask: -1 free var, 0 not acquired, 1 acquired\n",
    "    is_free = np.where(np.array(mask)==-1, True, False)\n",
    "    \n",
    "    is_free_df = df[:, is_free].copy()\n",
    "    is_costly_df = df[:, ~is_free].copy()\n",
    "    \n",
    "    mask_costly = [False if m == 1 else True for m in mask if m >= 0]\n",
    "    mask_df = np.zeros(is_costly_df.shape)\n",
    "    mask_df[:, mask_costly] = 1\n",
    "    \n",
    "    is_costly_df = np.where(mask_df == 1, 0, is_costly_df)\n",
    "    final_df = np.concatenate([is_costly_df, mask_df], axis=1)\n",
    "    final_df = np.concatenate([is_free_df, final_df], axis=1)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euthyroid <a name=\"euthyroid\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings <a name=\"exp_set\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data:\n",
    "data_dir = '../../data/final_data/'\n",
    "\n",
    "file_name = 'euthyroid.pkl'\n",
    "with open(data_dir + file_name, 'rb') as inp:\n",
    "    euth = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify utility matrix, betas:\n",
    "U = np.array([[0, -10],\n",
    "              [-1, 0]])\n",
    "\n",
    "betas = [20, 30, 50, 100]\n",
    "\n",
    "# Split the data to train and test sample:\n",
    "X_train, X_test, y_train, y_test = train_test_split(euth['features'], euth['targets'],\n",
    "                                                    test_size=0.25, random_state=42, \n",
    "                                                    stratify=euth['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train a single classifier on masked data:\n",
    "z_init = np.where(euth['costs'] == 0, -1, 0)\n",
    "\n",
    "train_X_dupl = get_boots_data(X_train, z_init, 16)\n",
    "train_y_dupl = np.concatenate([y_train] * 16)\n",
    "\n",
    "params = {'max_depth': 11,\n",
    "          'criterion': 'entropy',\n",
    "          'min_samples_split': 15,\n",
    "          'min_samples_leaf': 5,\n",
    "          'n_estimators': 500,\n",
    "          'random_state': 42}\n",
    "\n",
    "clf_boots = RandomForestClassifier(**params)\n",
    "\n",
    "clf_boots.fit(train_X_dupl, train_y_dupl)\n",
    "clf_boots = NodeClassifier(clf_boots, 'external')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with decision maker <a name=\"exp_dm\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Experiment with several combinations of decision maker:\n",
    "results = []\n",
    "acquisition_paths = {}\n",
    "for clf_mode in ['internal', 'external']:\n",
    "    for depth in [-1, 1]:\n",
    "        for beta in betas:\n",
    "            info = {'clf_mode': clf_mode, 'depth': depth, 'beta': beta}\n",
    "            print(' >>>> {}'.format(info))\n",
    "\n",
    "            if clf_mode == 'internal':\n",
    "                dm = DecisionMaker(depth=depth,\n",
    "                                   classifier_mode = clf_mode,\n",
    "                                   classifier_class=RandomForestClassifier,\n",
    "                                   classifier_params={'max_depth': 10,\n",
    "                                                      'criterion': 'entropy',\n",
    "                                                      'min_samples_split': 15,\n",
    "                                                      'min_samples_leaf': 5,\n",
    "                                                      'n_estimators': 200, \n",
    "                                                      'random_state': 42},\n",
    "                                   policy_class=RandomForestRegressor,\n",
    "                                   policy_params={'max_depth': 5,\n",
    "                                                 'min_samples_split': 15,\n",
    "                                                 'min_samples_leaf': 5,\n",
    "                                                 'n_estimators': 50, \n",
    "                                                 'random_state': 42})\n",
    "            else:\n",
    "                dm = DecisionMaker(depth=depth,\n",
    "                                   classifier_mode=clf_mode,\n",
    "                                   classifier_boots=clf_boots,\n",
    "                                   policy_class=RandomForestRegressor,\n",
    "                                   policy_params={'max_depth': 5,\n",
    "                                                  'min_samples_split': 15,\n",
    "                                                  'min_samples_leaf': 5,\n",
    "                                                  'n_estimators': 50, \n",
    "                                                  'random_state': 42})\n",
    "                \n",
    "            dm.fit(X_train, y_train, euth['costs'], beta*U)\n",
    "            prob_test = dm.predict(X_test)\n",
    "            test_evals = dm.evaluate(y_test)\n",
    "\n",
    "            for k in test_evals:\n",
    "                info['test_{}'.format(k)] = np.mean(test_evals[k])\n",
    "            \n",
    "            if clf_mode == 'internal' and depth == -1:\n",
    "                acquisition_paths[beta] = dm.acquisition_paths.copy()\n",
    "\n",
    "            prob_train = dm.predict(X_train)\n",
    "            train_evals = dm.evaluate(y_train)\n",
    "            for k in train_evals:\n",
    "                info['train_{}'.format(k)] = np.mean(train_evals[k])\n",
    "                \n",
    "            results.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(results)\n",
    "pdf['test_pred_u_scaled'] = pdf['test_prediction_utility'] / pdf['beta']\n",
    "pdf['train_pred_u_scaled'] = pdf['train_prediction_utility'] / pdf['beta']\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot peformances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf['key'] = pdf['clf_mode'] + '_' + pdf['depth'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(y=\"test_pred_u_scaled\", x=\"test_costs\", data=pdf, hue=\"key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(y=\"train_pred_u_scaled\", x=\"train_costs\", data=pdf, hue=\"key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### look at stored acquition paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for beta, path in acquisition_paths.items():\n",
    "    print('BETA: {}'.format(beta))\n",
    "    unq, cnt = np.unique(path, axis=0, return_counts=True)\n",
    "    for u, c in zip(unq, cnt):\n",
    "        print(' > {}: {}'.format(c, u[-4:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All and free variables <a name=\"exp_all\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_params={'max_depth': 10,\n",
    "                  'criterion': 'entropy',\n",
    "                  'min_samples_split': 15,\n",
    "                  'min_samples_leaf': 5,\n",
    "                  'n_estimators': 200, \n",
    "                  'random_state': 42}\n",
    "\n",
    "\n",
    "clf_all = RandomForestClassifier(**classifier_params)\n",
    "clf_all.fit(X_train, y_train)\n",
    "\n",
    "for X, y in zip([X_train, X_test], [y_train, y_test]):\n",
    "    prob = clf_all.predict_proba(X)\n",
    "    eu = np.matmul(prob, np.transpose(U))\n",
    "    predicted_classes = np.argmax(eu, axis=1)\n",
    "    u = U[predicted_classes, y]\n",
    "    print(np.mean(u)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(euth['costs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### free variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_free = np.where(euth['costs'] == 0, True, False)\n",
    "classifier_params={'max_depth': 10,\n",
    "                  'criterion': 'entropy',\n",
    "                  'min_samples_split': 15,\n",
    "                  'min_samples_leaf': 5,\n",
    "                  'n_estimators': 200, \n",
    "                  'random_state': 42}\n",
    "\n",
    "clf_all = RandomForestClassifier(**classifier_params)\n",
    "clf_all.fit(X_train[:, is_free], y_train)\n",
    "\n",
    "for X, y in zip([X_train[:, is_free], X_test[:, is_free]], [y_train, y_test]):\n",
    "    prob = clf_all.predict_proba(X)\n",
    "    eu = np.matmul(prob, np.transpose(U))\n",
    "    predicted_classes = np.argmax(eu, axis=1)\n",
    "    u = U[predicted_classes, y]\n",
    "    print(np.mean(u)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHANES <a name=\"nhanes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings <a name=\"nh_set\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data:\n",
    "data_dir = '../../data/final_data/'\n",
    "\n",
    "file_name = 'diabetes_all.pkl'\n",
    "with open(data_dir + file_name, 'rb') as inp:\n",
    "    diab = pickle.load(inp)\n",
    "    \n",
    "diab['costs'] = np.squeeze(diab['costs'])\n",
    "diab['targets'] = diab['targets'].astype('int64')\n",
    "\n",
    "# consider only costs associated with examination and laboratory tests\n",
    "z_init = np.where(diab['costs'] < 5, -1, 0)\n",
    "is_free = np.where(diab['costs'] < 5, True, False)\n",
    "costs = np.where(diab['costs'] < 5, 0, diab['costs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the utility matrix and betas:\n",
    "U = np.array([[0, -10, -20],\n",
    "              [-1, 0, -10],\n",
    "              [-2, -1, 0]])\n",
    "\n",
    "betas = [10, 15, 30, 50, 100]\n",
    "\n",
    "# Split data to test and train:\n",
    "X_train, X_test, y_train, y_test = train_test_split(diab['features'], diab['targets'],\n",
    "                                                    test_size=0.3, random_state=42, \n",
    "                                                    stratify=diab['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on masked data (the code runs for a long time ~1 hour): \n",
    "train_X_dupl = get_boots_data(X_train, z_init, 10)\n",
    "train_y_dupl = np.concatenate(10 * [y_train])\n",
    "\n",
    "params = {'max_depth': 12,\n",
    "          'criterion': 'entropy',\n",
    "          'min_samples_split': 60,\n",
    "          'min_samples_leaf': 20,\n",
    "          'n_estimators': 1000,\n",
    "          'random_state': 42}\n",
    "\n",
    "clf_boots = RandomForestClassifier(**params)\n",
    "\n",
    "clf_boots.fit(train_X_dupl, train_y_dupl)\n",
    "clf_boots = NodeClassifier(clf_boots, 'external')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model:\n",
    "with open('clf_boots.pkl', 'wb') as file:\n",
    "    pickle.dump(clf_boots, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model:\n",
    "with open('clf_boots.pkl', 'rb') as file:\n",
    "    clf_boots = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with decision maker <a name=\"nh_dm\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for clf_mode in ['internal', 'external']:\n",
    "    for depth in [1]:\n",
    "        for beta in betas:\n",
    "            info = {'clf_mode': clf_mode, 'depth': depth, 'beta': beta}\n",
    "\n",
    "            if clf_mode == 'internal':\n",
    "                dm = DecisionMaker(depth=depth,\n",
    "                                   classifier_mode = clf_mode,\n",
    "                                   classifier_class=RandomForestClassifier,\n",
    "                                   classifier_params={'max_depth': 11,\n",
    "                                                      'criterion': 'entropy',\n",
    "                                                      'min_samples_split': 60,\n",
    "                                                      'min_samples_leaf': 20,\n",
    "                                                      'n_estimators': 300, \n",
    "                                                      'random_state': 42},\n",
    "                                   policy_class=RandomForestRegressor,\n",
    "                                   policy_params={'max_depth': 5,\n",
    "                                                 'min_samples_split': 300,\n",
    "                                                 'min_samples_leaf': 100,\n",
    "                                                 'n_estimators': 10, \n",
    "                                                 'random_state': 42})\n",
    "            else:\n",
    "                dm = DecisionMaker(depth=depth,\n",
    "                                   classifier_mode=clf_mode,\n",
    "                                   classifier_boots=clf_boots,\n",
    "                                   policy_class=RandomForestRegressor,\n",
    "                                   policy_params={'max_depth': 5,\n",
    "                                                  'min_samples_split': 300,\n",
    "                                                  'min_samples_leaf': 100,\n",
    "                                                  'n_estimators': 10, \n",
    "                                                  'random_state': 42})\n",
    "\n",
    "                \n",
    "            dm.fit(X_train, y_train, costs, beta*U)\n",
    "            prob_test = dm.predict(X_test)\n",
    "            test_evals = dm.evaluate(y_test)\n",
    "            for k in test_evals:\n",
    "                info['test_{}'.format(k)] = np.mean(test_evals[k])\n",
    "                \n",
    "            prob_train = dm.predict(X_train)\n",
    "            train_evals = dm.evaluate(y_train)\n",
    "            for k in train_evals:\n",
    "                info['train_{}'.format(k)] = np.mean(train_evals[k])\n",
    "                \n",
    "            results.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(results)\n",
    "pdf['test_pred_u_scaled'] = pdf['test_prediction_utility'] / pdf['beta']\n",
    "pdf['train_pred_u_scaled'] = pdf['train_prediction_utility'] / pdf['beta']\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe the results:\n",
    "with open('diabetes_results_2.pkl', 'wb') as file:\n",
    "    pickle.dump(pdf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All and free varaibles <a name=\"nh_all\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_params={'max_depth': 11,\n",
    "                   'criterion': 'entropy',\n",
    "                   'min_samples_split': 60,\n",
    "                   'min_samples_leaf': 20,\n",
    "                   'n_estimators': 300, \n",
    "                   'random_state': 42}\n",
    "\n",
    "\n",
    "clf_all = RandomForestClassifier(**classifier_params)\n",
    "clf_all.fit(X_train, y_train)\n",
    "\n",
    "for X, y in zip([X_train, X_test], [y_train, y_test]):\n",
    "    prob = clf_all.predict_proba(X)\n",
    "    eu = np.matmul(prob, np.transpose(U))\n",
    "    predicted_classes = np.argmax(eu, axis=1)\n",
    "    u = U[predicted_classes, y]\n",
    "    print(np.mean(u)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(diab['costs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_free = np.where(costs == 0, True, False)\n",
    "classifier_params={'max_depth': 11,\n",
    "                   'criterion': 'entropy',\n",
    "                   'min_samples_split': 60,\n",
    "                   'min_samples_leaf': 20,\n",
    "                   'n_estimators': 300, \n",
    "                   'random_state': 42}\n",
    "\n",
    "clf_all = RandomForestClassifier(**classifier_params)\n",
    "clf_all.fit(X_train[:, is_free], y_train)\n",
    "\n",
    "for X, y in zip([X_train[:, is_free], X_test[:, is_free]], [y_train, y_test]):\n",
    "    prob = clf_all.predict_proba(X)\n",
    "    eu = np.matmul(prob, np.transpose(U))\n",
    "    predicted_classes = np.argmax(eu, axis=1)\n",
    "    u = U[predicted_classes, y]\n",
    "    print(np.mean(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clf_boots.pkl', 'rb') as file:\n",
    "    clf = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_all = np.where(costs==0, -1, 1)\n",
    "X_train_mask = mask_df(X_train, z_all)\n",
    "X_test_mask = mask_df(X_test, z_all)\n",
    "\n",
    "for X, y in zip([X_train_mask, X_test_mask], [y_train, y_test]):\n",
    "    prob = clf.classifier.predict_proba(X)\n",
    "    eu = np.matmul(prob, np.transpose(U))\n",
    "    predicted_classes = np.argmax(eu, axis=1)\n",
    "    u = U[predicted_classes, y]\n",
    "    print(np.mean(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_free = np.where(costs==0, -1, 0)\n",
    "X_train_mask = mask_df(X_train, z_free)\n",
    "X_test_mask = mask_df(X_test, z_free)\n",
    "\n",
    "for X, y in zip([X_train_mask, X_test_mask], [y_train, y_test]):\n",
    "    prob = clf.classifier.predict_proba(X)\n",
    "    eu = np.matmul(prob, np.transpose(U))\n",
    "    predicted_classes = np.argmax(eu, axis=1)\n",
    "    u = U[predicted_classes, y]\n",
    "    print(np.mean(u))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
